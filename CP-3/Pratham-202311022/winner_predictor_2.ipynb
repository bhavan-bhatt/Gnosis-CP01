{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import plot_importance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from skopt import BayesSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'H:/work_projects/College/DM_Projects/course_project_3/odi_Matches_Data/man_odi_data_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_list = set([])\n",
    "for cell in df['Team1 Playing 11']:\n",
    "    lst = cell.strip(\"[]\").replace(\"'\", \"\").replace(' ', '').split(',')\n",
    "    lst = [int(x) for x in lst]\n",
    "    for player in lst:\n",
    "        player_list.add(player)\n",
    "player_list = list(player_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(player_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_list = set([])\n",
    "for cell in df['Team2 Playing 11']:\n",
    "    lst = cell.strip(\"[]\").replace(\"'\", \"\").replace(' ', '').split(',')\n",
    "    lst = [int(x) for x in lst]\n",
    "    for player in lst:\n",
    "        player_list.add(player)\n",
    "player_list = list(player_list)\n",
    "len(player_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_1_players_df = pd.DataFrame(columns=['team1_P1','team1_P2','team1_P3','team1_P4','team1_P5','team1_P6','team1_P7','team1_P8','team1_P9','team1_P10','team1_P11'])\n",
    "for x in df['Team1 Playing 11']:\n",
    "    arr = np.array(x.replace('[', '').replace(']', '').replace(\"'\", \"\").split(', '))\n",
    "    arr = arr.astype('int')\n",
    "    team_1_players_df.loc[len(team_1_players_df)] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_2_players_df = pd.DataFrame(columns=['team2_P1','team2_P2','team2_P3','team2_P4','team2_P5','team2_P6','team2_P7','team2_P8','team2_P9','team2_P10','team2_P11'])\n",
    "for x in df['Team2 Playing 11']:\n",
    "    arr = np.array(x.replace('[', '').replace(']', '').replace(\"'\", \"\").split(', '))\n",
    "    arr = arr.astype('int')\n",
    "    team_2_players_df.loc[len(team_2_players_df)] = arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, team_1_players_df, team_2_players_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner = []\n",
    "for match_win, team_one in zip(df['Match Winner'], df['Team1 Name']):\n",
    "    if match_win == team_one:\n",
    "        winner.append(0)\n",
    "    else:\n",
    "        winner.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['match_winner'] = winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Match Winner', 'Toss Winner'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Debut Players', 'Team1 Playing 11', 'Team2 Playing 11'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Match Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data=df, columns=['Team1 Name', 'Team1 Captain', 'Team2 Name', 'Team2 Captain', 'Match Venue (Stadium)', 'Match Venue (City)',\n",
    "                                       'Match Venue (Country)', 'Toss Winner Choice'])\n",
    "le = LabelEncoder()\n",
    "lst = ['team1_P1','team1_P2','team1_P3','team1_P4','team1_P5','team1_P6','team1_P7','team1_P8','team1_P9','team1_P10','team1_P11','team2_P1','team2_P2','team2_P3','team2_P4','team2_P5','team2_P6','team2_P7','team2_P8','team2_P9','team2_P10','team2_P11']\n",
    "for i in lst:\n",
    "    df[i] = le.fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['match_winner'], axis=1)\n",
    "y = df['match_winner']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-1 with default params and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model0 = XGBClassifier(\n",
    "#     objective='binary:logistic',\n",
    "#     booster='gbtree',\n",
    "#     eval_metric='auc',\n",
    "#     tree_method='hist',\n",
    "#     # device='cuda',\n",
    "#     grow_policy='lossguide',\n",
    "#     use_label_encoder=False\n",
    "# )\n",
    "# model0.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {}\n",
    "dparams = model0.get_params()\n",
    "\n",
    "for key in dparams.keys():\n",
    "    gp = dparams[key]\n",
    "    default_params[key] = [gp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf0 = GridSearchCV(estimator=model0, scoring='accuracy', param_grid=default_params, verbose=3, cv=10, refit=True)\n",
    "# clf0.fit(x_train, y_train)\n",
    "# predictions = clf0.predict(x_test)\n",
    "# print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Params of Model-1\n",
    "bp = clf0.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-2 with Grid Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'gamma': [12.8,25.6,51.2,102.4, 200],\n",
    "              'learning_rate': [0.01, 0.03, 0.06, 0.1, 0.15, 0.2, 0.25, 0.300000012, 0.4, 0.5, 0.6, 0.7],\n",
    "              'max_depth': [5,6,7,8,9,10,11,12,13,14],\n",
    "              'n_estimators': [50,65,80,100,115,130,150],\n",
    "              'reg_alpha': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4,200],\n",
    "              'reg_lambda': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4,200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model0 = XGBClassifier(\n",
    "#     objective='binary:logistic',\n",
    "#     booster='gbtree',\n",
    "#     eval_metric='auc',\n",
    "#     tree_method='hist',\n",
    "#     device='cuda',\n",
    "#     grow_policy='lossguide',\n",
    "#     use_label_encoder=False\n",
    "# )\n",
    "\n",
    "# clf = RandomizedSearchCV(n_iter=500, estimator=model0, param_distributions=param_grid, scoring='accuracy', verbose=3, cv=10, refit=True)\n",
    "# clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = clf.predict(x_test)\n",
    "# print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67       363\n",
      "           1       0.65      0.61      0.63       347\n",
      "\n",
      "    accuracy                           0.65       710\n",
      "   macro avg       0.65      0.65      0.65       710\n",
      "weighted avg       0.65      0.65      0.65       710\n",
      "\n",
      "0.6492957746478873\n"
     ]
    }
   ],
   "source": [
    "file_name = 'xgb_base.pkl'\n",
    "xgb_bayesian = pickle.load(open(file_name, 'rb'))\n",
    "predictions = xgb_bayesian.predict(x_test)\n",
    "print(classification_report(predictions, y_test))\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.69      0.54       248\n",
      "           1       0.76      0.53      0.63       462\n",
      "\n",
      "    accuracy                           0.59       710\n",
      "   macro avg       0.60      0.61      0.58       710\n",
      "weighted avg       0.65      0.59      0.59       710\n",
      "\n",
      "0.5859154929577465\n"
     ]
    }
   ],
   "source": [
    "file_name = 'xgb_random_tuned.pkl'\n",
    "xgb_bayesian = pickle.load(open(file_name, 'rb'))\n",
    "predictions = xgb_bayesian.predict(x_test)\n",
    "print(classification_report(predictions, y_test))\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       373\n",
      "           1       0.63      0.61      0.62       337\n",
      "\n",
      "    accuracy                           0.64       710\n",
      "   macro avg       0.64      0.64      0.64       710\n",
      "weighted avg       0.64      0.64      0.64       710\n",
      "\n",
      "0.643661971830986\n"
     ]
    }
   ],
   "source": [
    "file_name = 'xgb_bayesian_tuned.pkl'\n",
    "xgb_bayesian = pickle.load(open(file_name, 'rb'))\n",
    "predictions = xgb_bayesian.predict(x_test)\n",
    "print(classification_report(predictions, y_test))\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after Tuning:\n",
    "\n",
    "Parameters that are tuned: \n",
    "\n",
    "- gamma\n",
    "- learning_rate\n",
    "- max_depth\n",
    "- n_estimators\n",
    "- reg_alpha\n",
    "- reg_lambda\n",
    "\n",
    "- Accuracy with default parameters:  0.6492957746478873\n",
    "- Accuracy with RandomSearch tuning: 0.5859154929577465\n",
    "- Accuracy with Bayesian tuning:     0.643661971830986"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
